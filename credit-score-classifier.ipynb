{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, SpectralClustering, DBSCAN\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.1: Exploration and Initial Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('train.csv')\n",
    "print(f'Data shape: {dataset.shape}\\n')\n",
    "print(f'Data features: \\n{dataset.dtypes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping irrelevent columns. We can just infer by common sense that these features don't affect the credit score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(columns = ['ID','Customer_ID','Name','SSN','Type_of_Loan'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2: Cleaning and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing unique values for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to help in identifying the unique values of a feature and its type to help in feature engineering\n",
    "def unique_vals_and_type(data: pd.DataFrame, feature: str, cap: int):\n",
    "  print(f'The unique values of \\'{feature}\\' feature of type {data[feature].dtype}: {dataset[feature].unique().tolist()[0:cap]}')\n",
    "\n",
    "for feature in dataset.columns:\n",
    "  unique_vals_and_type(dataset, feature, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining some helper functions to use in analysis and cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes special characters (underscores) from numeric data while ignoring nan values\n",
    "def remove_underscores_numeric(data: pd.DataFrame, feature: str, remove_neg: bool = False):\n",
    "  if data[feature].dtype == 'object':\n",
    "    data[feature] = pd.to_numeric(data[feature].str.replace('_', ''), errors = 'coerce')\n",
    "  if remove_neg: \n",
    "    data.loc[data[feature] < 0, feature] = np.nan\n",
    "\n",
    "# Counts outliers which are k standard deviations away from the mean\n",
    "def count_outliers(data: pd.DataFrame, feature: str, k: int):\n",
    "  m,s = data[feature].mean() , data[feature].std()\n",
    "  return np.sum((data[feature] < m - k*s) | (data[feature] > m + k*s))\n",
    "\n",
    "# Clips outliers which are k standard deviations away from the mean\n",
    "def clip_outliers(data: pd.DataFrame, feature: str, k: int):\n",
    "  m,s = data[feature].mean() , data[feature].std()\n",
    "  data[feature] = data[feature].clip(lower = m - k*s, upper = m + k*s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert numerical features from 'object' type to their orignal type and replacing missing data of each with NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_underscores_numeric(dataset, 'Age', remove_neg = True)\n",
    "remove_underscores_numeric(dataset, 'Num_of_Loan', remove_neg = True)\n",
    "remove_underscores_numeric(dataset, 'Num_of_Delayed_Payment')\n",
    "remove_underscores_numeric(dataset, 'Num_Bank_Accounts', remove_neg = True)\n",
    "remove_underscores_numeric(dataset, 'Annual_Income')\n",
    "remove_underscores_numeric(dataset, 'Changed_Credit_Limit')\n",
    "remove_underscores_numeric(dataset, 'Outstanding_Debt')\n",
    "remove_underscores_numeric(dataset, 'Amount_invested_monthly')\n",
    "remove_underscores_numeric(dataset, 'Monthly_Balance')\n",
    "remove_underscores_numeric(dataset, 'Num_of_Delayed_Payment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing erroneous values in text data with NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Occupation'] = dataset['Occupation'].replace(to_replace = '_______', value = np.nan)\n",
    "dataset['Credit_Mix'] = dataset['Credit_Mix'].replace(to_replace = '_', value = np.nan)\n",
    "dataset['Payment_of_Min_Amount'] = dataset['Payment_of_Min_Amount'].replace(to_replace = 'NM', value = np.nan)\n",
    "dataset['Payment_Behaviour'] = dataset['Payment_Behaviour'].replace(to_replace = '!@9#%8', value = np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting data from text columns which have embedded values in a certain form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the Credit History Age column with years and months.\n",
    "temp_df = dataset['Credit_History_Age'].str.extract(r'(?P<Years>[\\d]?[\\d]) Years and (?P<Months>[\\d]?[\\d]) Months', expand = True)\n",
    "dataset['Credit_History_Age'] = temp_df['Years'].astype('float64') + (temp_df['Months'].astype('float64') / 12)\n",
    "\n",
    "# Split the Payement_Behavior feature into two features which will replace the original feature\n",
    "temp_df = dataset['Payment_Behaviour'].str.extract(r'(?P<Spending_Behavior>High|Low)_spent_(?P<Payment_Volume>Small|Medium|Large)_value_payments', expand = True)\n",
    "dataset = pd.concat([dataset.drop(columns = ['Payment_Behaviour']), temp_df], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking unique values for each categorical column after cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get which features are of 'object' type in the dataset and their unique values\n",
    "for feature in dataset.select_dtypes(include = 'object').columns:\n",
    "  unique_vals_and_type(dataset, feature, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decide whether the occupation and Month columns should be kept, we plot a histogram of occupation - credit score and another one with month - credit score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a cluster column chart of occupation vs credit score\n",
    "counts = dataset.groupby(['Occupation','Credit_Score'], dropna = True).size().unstack()\n",
    "counts.plot(kind = 'bar', stacked = False, figsize = (10, 5))\n",
    "plt.xlabel('Occupation')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Credit_Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a cluster column chart of month vs credit score\n",
    "counts = dataset.groupby(['Month','Credit_Score'], dropna = True).size().unstack()\n",
    "counts.plot(kind = 'bar', stacked = False, figsize = (10, 5))\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Credit_Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of credit score for each occupation and for each month are nearly the same, so occupation & month values won't be significant for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'Occupation' and 'Month' columns\n",
    "dataset.drop(columns = ['Occupation','Month'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.3: Encoding and Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll encode each categorical variable by a numerical ordinal encoding.  \n",
    "For this notebook, we will map the values before the train-test split, but it will be easy to construct a custom mapper class to take in a dataframe similar to the original and transform its values to the specified mapping before classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping categorical features and grouping numeric features for future use\n",
    "cat_features = dataset.select_dtypes(include = 'object')\n",
    "num_features = dataset.select_dtypes(exclude = 'object')\n",
    "cat_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the categorical variables in a separate dataframe\n",
    "encoder = OrdinalEncoder(\n",
    "  categories = [['Bad','Standard','Good'],['No','Yes'],['Poor','Standard','Good'],['Low','High'],['Small','Medium','Large']],\n",
    "  handle_unknown = 'use_encoded_value',\n",
    "  unknown_value = np.nan\n",
    ")\n",
    "encoded_cat_features = pd.DataFrame(data = encoder.fit_transform(cat_features), columns = cat_features.columns)\n",
    "encoded_cat_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the encoded data\n",
    "encoded_data = pd.concat([dataset.drop(columns = cat_features.columns), encoded_cat_features], axis = 1)\n",
    "encoded_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the dataset to see summaries of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the original data for NA value and outlier percentage. We will process the encoded data accordingly after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_na_and_outliers(dataset: pd.DataFrame):\n",
    "  # Count outliers and NA values for each numerical feature and store results in a dataframe\n",
    "  features, outliers, percent_outliers, nas, percent_nas = [],[],[],[],[]\n",
    "\n",
    "  for f in dataset.columns:\n",
    "    features.append(f)\n",
    "    if (dataset[f].dtype != 'object'):\n",
    "      outliers.append(count_outliers(dataset, f, 3))\n",
    "      percent_outliers.append(outliers[-1]/dataset.shape[0] * 100)\n",
    "    else:\n",
    "      outliers.append(np.nan)\n",
    "      percent_outliers.append(np.nan)\n",
    "    nas.append(dataset[f].isna().sum())\n",
    "    percent_nas.append(nas[-1]/dataset.shape[0] * 100)\n",
    "\n",
    "  return pd.DataFrame({'Feature': features, 'Outliers': outliers, 'Outlier%': percent_outliers, 'NA': nas, 'NA%': percent_nas})\n",
    "\n",
    "count_na_and_outliers(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All outliers seem to be within the 5% range of the whole dataset, so we will leave them as they might belong to the original distribution. On the other hand, NA values will cause problems, and will incur a noticable loss of information if we just delete the rows or columns which have NA values in all features, so we will perform data imputation for these values.  \n",
    "We will remove entries with NA Age or Num_Bank_Accounts as these are not many and won't have a significant effect on the model, then, we will use simple imputation by most frequent value for categorical data and iterative imputation for numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rows with NA Age or Num_Bank_Accounts\n",
    "imputed_data = encoded_data.copy().dropna(subset = ['Age','Num_Bank_Accounts'])\n",
    "\n",
    "# Performing data imputation for categorical data\n",
    "cat_imputer = SimpleImputer(strategy = 'most_frequent')\n",
    "cat_list = ['Credit_Mix','Payment_of_Min_Amount','Spending_Behavior','Payment_Volume']\n",
    "imputed_data[cat_list] = cat_imputer.fit_transform(imputed_data[cat_list])\n",
    "\n",
    "# Performing data imputation for numerical data\n",
    "num_imputer = IterativeImputer(max_iter = 10, random_state = 42)\n",
    "num_list = ['Monthly_Inhand_Salary','Num_of_Loan','Num_of_Delayed_Payment','Changed_Credit_Limit','Num_Credit_Inquiries','Credit_History_Age','Amount_invested_monthly','Monthly_Balance']\n",
    "imputed_data[num_list] = num_imputer.fit_transform(imputed_data[num_list])\n",
    "\n",
    "# Analyzing NA and Outlier counts for the new data\n",
    "count_na_and_outliers(imputed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1: Splitting The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2: Normalizing The Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.3: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Testing & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.1: Running The Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.2: Evaluation Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.3: Evaluation Results' Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
